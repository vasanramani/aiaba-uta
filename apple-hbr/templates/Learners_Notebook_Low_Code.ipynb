{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CNz35ia6Bz3"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRbhMJH6Bz3"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PBm5xaj6Bz3"
      },
      "source": [
        "As organizations grow, business analysts increasingly face the challenge of navigating large volumes of reports, research papers, and strategic documents. Extracting the right insights from lengthy materials can be time-consuming and overwhelming, especially when these insights directly influence key business decisions.\n",
        "\n",
        "Consider joining a venture capital firm like Andreessen Horowitz and being assigned a dense report such as Harvard Business Review’s **“How Apple is Organized for Innovation.”** Manually reviewing such documents requires significant effort, slowing down the analysis process and increasing the chances of missing important details.\n",
        "\n",
        "To overcome this information overload, businesses can leverage **Semantic Search** and **Retrieval-Augmented Generation (RAG)** models. These systems allow analysts to ask natural-language questions like, *“How does Apple structure its teams for innovation?”* and instantly retrieve relevant, accurate insights from the source document.\n",
        "\n",
        "By integrating such AI-driven retrieval systems, organizations can streamline research workflows, reduce manual effort, and enable analysts to focus on high-value strategic thinking, ultimately improving decision-making speed and quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xDPsqvO6Bz5"
      },
      "source": [
        "**Common Questions to Answer**\n",
        "\n",
        "1. Who are the authors of this article and who published this article?\n",
        "\n",
        "2. List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\n",
        "\n",
        "3. Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CARPKFwm6Bz4"
      },
      "source": [
        "### Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOElOEXq6Bz4"
      },
      "source": [
        "As an AI specialist, your task is to develop a RAG-based application that enables business analysts to efficiently extract insights from extensive business reports such as “How Apple is Organized for Innovation.” The objective is to understand the challenges of navigating long, information-dense documents, apply retrieval-augmented generation techniques to surface only the most relevant content, analyze how this improves the speed and accuracy of report interpretation, evaluate its potential to enhance strategic decision-making and productivity for analysts, and create a functional prototype that demonstrates the system’s effectiveness in answering queries, summarizing key insights, and supporting natural-language interactions without requiring users to read the entire report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by9EvAnkSpZf"
      },
      "source": [
        "### Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KagUxRh3lKiv"
      },
      "source": [
        "**“How Apple is Organized for Innovation”** is a detailed Harvard Business Review article that examines Apple’s unique approach to structuring teams, driving innovation, and maintaining a culture of excellence. The article is provided as a PDF consisting of **11 pages**, offering in-depth insights into Apple’s organizational design, leadership principles, and decision-making processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Please read the instructions carefully before starting the project.**"
      ],
      "metadata": {
        "id": "EWGlpDNkrTqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks '_____' are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every '_____' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same. Any mathematical or computational details which are a graded part of the project can be included in the Appendix section of the presentation."
      ],
      "metadata": {
        "id": "oIAAXUOip1Fc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnwETBOE6Bz5"
      },
      "source": [
        "## Installing and Importing Necessary Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q4GgLhZhUM4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d5db20-9315-4327-b57f-6b671d8915bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "langgraph-prebuilt 1.0.7 requires langchain-core>=1.0.0, but you have langchain-core 0.3.83 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.23.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q langchain_community==0.3.27 \\\n",
        "              langchain==0.3.27 \\\n",
        "              chromadb==1.0.15 \\\n",
        "              pymupdf==1.26.3 \\\n",
        "              tiktoken==0.9.0 \\\n",
        "              datasets==4.0.0 \\\n",
        "              evaluate==0.4.5 \\\n",
        "              langchain_openai==0.3.30"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ],
      "metadata": {
        "id": "mDp-EYZH-69E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTY9GN4oWK3g"
      },
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import os                                                                       # Interact with the operating system (e.g., set environment variables)\n",
        "import json                                                                     # Read/write JSON data\n",
        "import requests  # type: ignore                                                 # Make HTTP requests (e.g., API calls); ignore type checker\n",
        "\n",
        "# Import libraries for working with PDFs and OpenAI\n",
        "from langchain.document_loaders import PyMuPDFLoader                            # Load and extract text from PDF files\n",
        "# from langchain_community.document_loaders import PyPDFLoader                    # Load and extract text from PDF files\n",
        "from openai import OpenAI                                                       # Access OpenAI's models and services\n",
        "\n",
        "# Import libraries for processing dataframes and text\n",
        "import tiktoken                                                                 # Tokenizer used for counting and splitting text for models\n",
        "import pandas as pd                                                             # Load, manipulate, and analyze tabular data\n",
        "\n",
        "# Import LangChain components for data loading, chunking, embedding, and vector DBs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter              # Break text into overlapping chunks for processing\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings                        # Create vector embeddings using OpenAI's models  # type: ignore\n",
        "from langchain.vectorstores import Chroma                                       # Store and search vector embeddings using Chroma DB  # type: ignore\n",
        "\n",
        "from datasets import Dataset                                                    # Used to structure the input (questions, answers, contexts etc.) in tabular format\n",
        "from langchain_openai import ChatOpenAI                                         # This is needed since LLM is used in metric computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtZWqj0wFTS1"
      },
      "source": [
        "## Question Answering using LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfNKCvuzWSI-"
      },
      "source": [
        "### OpenAI API Calling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** <br> Make sure to create a `config.json` file in your project directory containing your OpenAI credentials in the following format:\n",
        "<br><br>```{\"API_KEY\": \"your_openai_api_key_here\",\"OPENAI_API_BASE\": \"your_api_base\"}```<br><br>\n",
        "Replace the placeholder with your actual API key. This file allows your script to securely load API configuration details without hardcoding them directly into the code. </br>\n"
      ],
      "metadata": {
        "id": "XwbbfYZpBjYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON file and extract values\n",
        "file_name = \"config.json\"                                                       # Name of the configuration file\n",
        "with open(file_name, 'r') as file:                                              # Open the config file in read mode\n",
        "    config = json.load(file)                                                    # Load the JSON content as a dictionary\n",
        "    OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")                                             # Extract the API key from the config\n",
        "    OPENAI_API_BASE = config.get(\"OPENAI_API_BASE\")                             # Extract the OpenAI base URL from the config\n",
        "\n",
        "# Store API credentials in environment variables\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY                                          # Set API key as environment variable\n",
        "os.environ[\"OPENAI_BASE_URL\"] = OPENAI_API_BASE                                 # Set API base URL as environment variable\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()                                                               # Create an instance of the OpenAI client"
      ],
      "metadata": {
        "id": "eMi5GjvNBrlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzzkvIXvFTS4"
      },
      "source": [
        "### Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9vjnCz0WSJC"
      },
      "outputs": [],
      "source": [
        "# Define a function to get a response\n",
        "def response(user_prompt, max_tokens=5, temperature=1, top_p=0.9):   # Complete the code to set default paramenters\n",
        "    # Create a chat completion using the OpenAI client\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",  # Complete the code by specifying the model to be used.\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": user_prompt}                            # User prompt is the input/query to respond to\n",
        "        ],\n",
        "        max_tokens=max_tokens,                                                  # Max number of tokens to generate in the response\n",
        "        temperature=temperature,                                                # Controls randomness in output\n",
        "        top_p=top_p                                                             # Controls diversity via nucleus sampling\n",
        "    )\n",
        "    return completion.choices[0].message.content                                # Return the text content from the model's reply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8YgK91SFjVY"
      },
      "source": [
        "### Question 1: Who are the authors of this article and who published this article?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2Q_QZ4OFjVa"
      },
      "outputs": [],
      "source": [
        "question_1 = \"Who are the authors of this article and who published this article ?\"\n",
        "base_prompt_response_1=response(question_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6yxICeVFjVc"
      },
      "source": [
        "### Question 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO1OTE9CFjVd"
      },
      "outputs": [],
      "source": [
        "question_2 = \"_____\" #Complete the code to define the question #2\n",
        "base_prompt_response_2=response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oflaoOGiFjVd"
      },
      "source": [
        "### Question 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFm5Tq7RFjVe"
      },
      "outputs": [],
      "source": [
        "question_3 = \"_____\" #Complete the code to define the question #3\n",
        "base_prompt_response_3=response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "-\n",
        "-"
      ],
      "metadata": {
        "id": "KQcOiXwSybZy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5myZ5dOOefc"
      },
      "source": [
        "## Question Answering using LLM with Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we will use prompt engineering to check the effect of a more detailed and well-engineered prompt on the output of the model."
      ],
      "metadata": {
        "id": "dHbFv8hO7Rjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "___\n",
        "\"\"\" #Complete the code to define the system prompt"
      ],
      "metadata": {
        "id": "VMZqTudYBCWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the function to Generate a Response From the LLM"
      ],
      "metadata": {
        "id": "X69OhTHAX9xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get a response from the OpenAI chat model\n",
        "def response(system_prompt, user_prompt, max_tokens=_____, temperature=______, top_p=_____):  # Complete the code to set default paramenters\n",
        "    # Create a chat completion using the OpenAI client\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"_______\",                                                        # Complete the code by specifying the model to be used.\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},                       # System prompt sets the assistant's behavior\n",
        "            {\"role\": \"user\", \"content\": user_prompt}                            # User prompt is the input/query to respond to\n",
        "        ],\n",
        "        max_tokens=max_tokens,                                                  # Max number of tokens to generate in the response\n",
        "        temperature=temperature,                                                # Controls randomness in output (0 = deterministic)\n",
        "        top_p=top_p                                                             # Controls diversity via nucleus sampling\n",
        "    )\n",
        "    return completion.choices[0].message.content                                # Return the text content from the model's reply"
      ],
      "metadata": {
        "id": "x5Wi_VwNkipi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jg3r_LWOeff"
      },
      "source": [
        "### Question 1: Who are the authors of this article and who published this article ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5zh3HQoOeff"
      },
      "outputs": [],
      "source": [
        "response_with_prompt_eng_1=response(system_prompt,question_1)\n",
        "response_with_prompt_eng_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYpyw4HjOeff"
      },
      "source": [
        "### Question 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPPpDM6cOeff"
      },
      "outputs": [],
      "source": [
        "response_with_prompt_eng_2=response(_____,_____) #Complete the code to pass the user prompt and system prompt\n",
        "response_with_prompt_eng_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRp92JQZOeff"
      },
      "source": [
        "### Question 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC6rrtblOefg"
      },
      "outputs": [],
      "source": [
        "response_with_prompt_eng_3=response(_____,_____) #Complete the code to pass the user prompt and system prompt\n",
        "response_with_prompt_eng_3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**:\n",
        "-\n",
        "-\n"
      ],
      "metadata": {
        "id": "kStw6hGgyhzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation for RAG"
      ],
      "metadata": {
        "id": "t_O1PGdNO2M9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTpWESc53dL9"
      },
      "source": [
        "### Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5dLjPsyi1A4"
      },
      "outputs": [],
      "source": [
        "# uncomment and run the below code snippets if the dataset is present in the Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksv9hSCR4BM_"
      },
      "outputs": [],
      "source": [
        "pdf_path = \"_____\" #Complete the code to define the file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhf34I1eYNtR"
      },
      "outputs": [],
      "source": [
        "pdf_loader = PyMuPDFLoader(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YChLS31TxC3-"
      },
      "outputs": [],
      "source": [
        "pdf = pdf_loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffj0ca3eZT4u"
      },
      "source": [
        "### Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9weTDzMxRRS"
      },
      "source": [
        "#### Checking the first 5 pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSOv3q2pxX4z"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print(f\"Page Number : {i+1}\",end=\"\\n\")\n",
        "    print(pdf[i].page_content,end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LECMxTH-zB-R"
      },
      "source": [
        "### Data Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chunk the PDF into Manageable Text Sections Using a Token-Based Splitter"
      ],
      "metadata": {
        "id": "oQfw-qErRoGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG0_pBmizGGt"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name='cl100k_base',\n",
        "    chunk_size=_____, #Complete the code to define the chunk size\n",
        "    chunk_overlap= _____ #Complete the code to define the chunk overlap\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split the Loaded PDF into Chunks for Further Processing"
      ],
      "metadata": {
        "id": "P7XqisNKR3DZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w76ji7ECzLLQ"
      },
      "outputs": [],
      "source": [
        "document_chunks = pdf_loader.load_and_split(text_splitter)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check the Number of Chunks Created"
      ],
      "metadata": {
        "id": "gkkBb1GmSDTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6TQ-mmLzR9I"
      },
      "outputs": [],
      "source": [
        "len(document_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKnhARSu0d8u"
      },
      "source": [
        "### Generate Vector Embeddings for Text Chunks Using OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6cZVZWQz15c",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Initialize the OpenAI Embeddings model with API credentials\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    openai_api_key=OPENAI_API_KEY,                                                     # Your OpenAI API key for authentication\n",
        "    openai_api_base=OPENAI_API_BASE                                             # The OpenAI API base URL endpoint\n",
        ")\n",
        "\n",
        "# Generate embeddings (vector representations) for the first two document chunks\n",
        "embedding_1 = embedding_model.embed_query(document_chunks[0].page_content)      # Embedding for chunk 0\n",
        "embedding_2 = embedding_model.embed_query(document_chunks[1].page_content)      # Embedding for chunk 1\n",
        "\n",
        "# Check and print the dimension (length) of the embedding vector\n",
        "print(\"Dimension of the embedding vector \", len(embedding_1))                   # Typically 1536 or 2048 depending on model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0qy6xOZ0UBe",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Verify if both embeddings have the same dimension (should be True)\n",
        "len(embedding_1) == len(embedding_2)\n",
        "\n",
        "# Return/display the two embedding vectors for further inspection or use\n",
        "embedding_1, embedding_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiKCOv4X0d7B"
      },
      "source": [
        "### Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup Vector Database Directory"
      ],
      "metadata": {
        "id": "_lKe-Yo6UzHL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWOqhGMV0kZ9"
      },
      "outputs": [],
      "source": [
        "out_dir = '____'    # complete the code to define the name of the vector database\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "  os.makedirs(out_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Vector Database from Documents"
      ],
      "metadata": {
        "id": "qvn1Q8bQVDDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g-D3URW0iEO"
      },
      "outputs": [],
      "source": [
        "# Building the vector store and saving it to disk for future use\n",
        "vectorstore = Chroma.from_documents(\n",
        "    document_chunks,                                                            # Documents to index\n",
        "    embedding_model,                                                            # Embedding model for converting text to vectors\n",
        "    persist_directory=out_dir                                                   # Save vector DB files here\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Vector Database"
      ],
      "metadata": {
        "id": "DoJ1Bqb2VWkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuK6hsbaGfqH"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma(\n",
        "    persist_directory=out_dir,\n",
        "    embedding_function=embedding_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explore Vector Database and Perform Searches"
      ],
      "metadata": {
        "id": "RtGfyOaeVlqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdZON_Uj1EeS"
      },
      "outputs": [],
      "source": [
        "vectorstore.embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write an instruction on what to do in next cell"
      ],
      "metadata": {
        "id": "dhXVQLa48mR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9HgsipF1I4H"
      },
      "outputs": [],
      "source": [
        "vectorstore.similarity_search(\"_____\",k=_____) #Complete the code to pass a query and an appropriate k value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uo9cym60X-U"
      },
      "source": [
        "### Retrieval and Response Generation using Vector Search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert Vector Database into a Retriever and Retrieve Relevant Documents"
      ],
      "metadata": {
        "id": "9zscYgoFfgXi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO5kmp381VsX"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': _____} #Complete the code to pass an appropriate k value\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw8qcwq66B0C",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### System and User Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wRkZYtO6B0D"
      },
      "source": [
        "Prompts guide the model to generate accurate responses. Here, we define two parts:\n",
        "\n",
        "    1. The system message describing the assistant's role.\n",
        "    2. A user message template including context and the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1737358838889
        },
        "id": "Dyl60SEs6B0D",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "qna_system_message = \"\"\"\n",
        "___\n",
        "\"\"\"  #Complete the code to define the system message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW38rWoNjJkQ"
      },
      "outputs": [],
      "source": [
        "qna_user_message_template = \"\"\"\n",
        "___\n",
        "\"\"\"  #Complete the code to define the user message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkIteX4m6mny"
      },
      "source": [
        "### Response Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-SfCZqC6B0E",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def generate_rag_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95):\n",
        "    global qna_system_message,qna_user_message_template\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "\n",
        "    # Combine document chunks into a single context\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "    user_message = qna_user_message_template.replace('{context}', context_for_query)\n",
        "    user_message = user_message.replace('{question}', user_input)\n",
        "\n",
        "    # Generate the response\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"_____\",   # Complete the code by specifying the model to be used.\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": qna_system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "        )\n",
        "        # Extract and print the generated text from the response\n",
        "        response = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering using RAG"
      ],
      "metadata": {
        "id": "ffP1SRYbPQHN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjajBEj06B0E"
      },
      "source": [
        "### Question 1: Who are the authors of this article and who published this article ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt4TAQNa6B0E"
      },
      "outputs": [],
      "source": [
        "response_with_rag_1 = generate_rag_response(question_1)\n",
        "response_with_rag_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDw8zXuq6B0F"
      },
      "source": [
        "### Question 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i92cv0dQ6B0F"
      },
      "outputs": [],
      "source": [
        "response_with_rag_2 = generate_rag_response(_____) #Complete the code to pass the user input\n",
        "response_with_rag_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TggYyQPL6B0G"
      },
      "source": [
        "### Question 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed6x6LGb6B0G"
      },
      "outputs": [],
      "source": [
        "response_with_rag_3 = generate_rag_response(_____) #Complete the code to pass the user input\n",
        "response_with_rag_3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observations**\n",
        "\n",
        "-\n",
        "-\n",
        "-"
      ],
      "metadata": {
        "id": "-_B3bu8m6oPD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7QICRU-njdj"
      },
      "source": [
        "## Actionable Insights and Business Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   \n",
        "*  \n",
        "*\n",
        "\n"
      ],
      "metadata": {
        "id": "ObyXYhOojIaY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybRlzaIhWaM9"
      },
      "source": [
        "<font size=6 color='#4682B4'>Power Ahead</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QgSCW-EIDBlA",
        "uTpWESc53dL9",
        "ffj0ca3eZT4u",
        "f9weTDzMxRRS",
        "LECMxTH-zB-R",
        "oQfw-qErRoGr",
        "P7XqisNKR3DZ",
        "gkkBb1GmSDTp",
        "ukLJz60rI78h"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}