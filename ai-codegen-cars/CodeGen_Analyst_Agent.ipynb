{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmKLmjqmHMQM"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=10>AI Agents for Business Applications</center></font>\n",
        "<center><font size=6>Week 1 - Introduction to AI Agents</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EgG25UQ3PX"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://images.pexels.com/photos/2653362/pexels-photo-2653362.jpeg\" width=\"640\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=6>CodeGen Analyst Agent</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CNz35ia6Bz3"
      },
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRbhMJH6Bz3"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PBm5xaj6Bz3"
      },
      "source": [
        "The used-car resale industry is becoming increasingly data-driven, with organizations handling large volumes of historical vehicle market data to support pricing, inventory, and market analysis decisions. The need for quick access to reliable insights from this data is critical for responding to market changes, optimizing inventory, and maintaining competitive pricing in a fast-paced environment.\n",
        "\n",
        "Data teams often face challenges in efficiently extracting insights from complex datasets that include multiple vehicle attributes such as model, year, mileage, fuel type, and transmission. Translating business questions into accurate data analysis code can be time-consuming, particularly when similar exploratory analyses are performed repeatedly. This challenge is amplified by growing expectations for faster turnaround times and consistent analytical outputs.\n",
        "\n",
        "To address these challenges, organizations can focus on adopting solutions that simplify data interaction and accelerate insight generation. Enabling natural language driven analysis allows data analyst teams to access data more intuitively, reduces manual effort, and supports timely, data-backed decision-making across business teams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CARPKFwm6Bz4"
      },
      "source": [
        "### Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOElOEXq6Bz4"
      },
      "source": [
        "The objective is to simplify and streamline the day-to-day work of data analysts by enabling natural language interaction with used-car market data. Common analytical requests can be translated into executable data analysis code, reducing the time spent on writing, debugging, and repeating similar logic. This allows data analysts to focus more on analysis and validation rather than manual coding.\n",
        "\n",
        "- Reduce time spent on repetitive data analysis tasks\n",
        "\n",
        "- Improve consistency and reliability of analysis code\n",
        "\n",
        "- Support faster turnaround for routine analytical requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by9EvAnkSpZf"
      },
      "source": [
        "### Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYz--0YHtb4F"
      },
      "source": [
        "The dataset consists of several key attributes describing vehicle listings:\n",
        "\n",
        "* **ID**: Unique identifier assigned to each vehicle listing.\n",
        "* **Region**: Categorical variable representing the broader geographic region where the vehicle is listed.\n",
        "* **Price**: Numerical value indicating the listed price of the vehicle.\n",
        "* **Year**: Numerical value representing the model year of the vehicle.\n",
        "* **Manufacturer**: Categorical variable identifying the vehicle’s manufacturer.\n",
        "* **Model**: Categorical variable specifying the model name of the vehicle.\n",
        "* **Cylinders**: Categorical variable describing the engine cylinder configuration (for example, 4 cylinders, 6 cylinders).\n",
        "* **Fuel**: Categorical variable indicating the type of fuel used by the vehicle (such as gas, diesel, electric, or hybrid).\n",
        "* **Odometer**: Numerical value representing the total distance the vehicle has been driven.\n",
        "* **Title Status**: Categorical variable indicating the legal title condition of the vehicle (for example, clean, salvage, rebuilt).\n",
        "* **Transmission**: Categorical variable specifying the type of transmission (automatic or manual).\n",
        "* **Drive**: Categorical variable describing the drivetrain configuration (for example, front-wheel drive, rear-wheel drive, all-wheel drive).\n",
        "* **Type**: Categorical variable indicating the vehicle body type (such as sedan, SUV, truck, or coupe).\n",
        "* **Paint Color**: Categorical variable representing the exterior color of the vehicle.\n",
        "* **Description**: Textual field containing the seller-provided description of the vehicle.\n",
        "* **State**: Categorical variable identifying the state in which the vehicle is listed.\n",
        "* **Latitude**: Numerical value representing the latitude of the listing location.\n",
        "* **Longitude**: Numerical value representing the longitude of the listing location.\n",
        "* **Posting Date**: Date field indicating when the vehicle listing was posted.\n",
        "\n",
        "This dataset captures a wide variety of vehicle characteristics, pricing information, and geographic details, enabling comprehensive analysis of used car markets and supporting tasks such as price prediction, demand analysis, and regional trend identification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnwETBOE6Bz5"
      },
      "source": [
        "## **Installing and Importing Necessary Libraries and Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzU8ZNrzqZ11",
        "outputId": "29eed524-8906-41b4-eef7-531d0c3e26e2"
      },
      "outputs": [],
      "source": [
        "!pip install -q langgraph \\\n",
        "                langchain \\\n",
        "                langchain-core \\\n",
        "                langchain-openai \\\n",
        "                langchain-community \\\n",
        "                grandalf \\\n",
        "                pandas \\\n",
        "                numpy \\\n",
        "                langchain-experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDp-EYZH-69E"
      },
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqo4JtU0tqOX",
        "outputId": "f6ef0947-7362-416e-bb8d-fc305ee86e6c"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Any ,Dict\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on Windows\n"
          ]
        }
      ],
      "source": [
        "# Display Markdown formatted string in the notebook output.\n",
        "# This function takes a string as input and uses IPython.display.Markdown\n",
        "# to render it as Markdown, providing a more visually appealing output.\n",
        "def printmd(string):\n",
        "    print(string)\n",
        "\n",
        "# Identify the environment to switch the file paths either to use Google Drive or Local\n",
        "import sys\n",
        "# Check if the operating system is Windows\n",
        "if sys.platform.startswith('win'):\n",
        "    printmd(\"Running on Windows\")\n",
        "    resourcePath=\"\"\n",
        "else:\n",
        "    # Assume it's a Linux-based environment, likely Google Colab\n",
        "    printmd(\"Running on Linux most likely Colab\")\n",
        "    # Mount Google Drive for Colab environments to access files\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    # Define the base path for resources in Google Drive\n",
        "    resourcePath=\"/content/drive/MyDrive/ColabAssignments/CodeGenCars/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq1lhM4WFTS2"
      },
      "source": [
        "## **Data Loading and Model Initialization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G2bFup5u6vE"
      },
      "source": [
        "### OpenAI API Calling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eMi5GjvNBrlO"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file and extract values\n",
        "file_name = resourcePath + \"config.json\"                                                       # Name of the configuration file\n",
        "with open(file_name, 'r') as file:                                              # Open the config file in read mode\n",
        "    config = json.load(file)                                                    # Load the JSON content as a dictionary\n",
        "    OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")                                             # Extract the API key from the config\n",
        "    OPENAI_API_BASE = config.get(\"OPENAI_API_BASE\")                             # Extract the OpenAI base URL from the config\n",
        "\n",
        "# Store API credentials in environment variables\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY                                  # Set API key as environment variable\n",
        "os.environ[\"OPENAI_BASE_URL\"] = OPENAI_API_BASE                                 # Set API base URL as environment variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovsiLYUd4EW2"
      },
      "source": [
        "For the problem at hands, we will use two LLMs to separate responsibilities\n",
        "- a lightweight model for primary reasoning and generation, and\n",
        "- a more complex model for evaluation and validation, ensuring better accuracy, reliability, and cost efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z0Gdm_e17rXP"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "evaluator_llm = ChatOpenAI(model='gpt-4o', temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krZUb4K8u8LF"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM9ybdIAvjEQ"
      },
      "source": [
        "Define data path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eYRs5JahvIx_"
      },
      "outputs": [],
      "source": [
        "data_path = resourcePath + \"used_car_dataset.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVvLROd7vluT"
      },
      "source": [
        "**Load CSV into DataFrame**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzLIRPk2yvkm"
      },
      "source": [
        "This line of code loads the CSV file into the notebook:\n",
        "\n",
        "* `pd.read_csv(data_path)` reads the CSV file located at the path stored in `data_path`.\n",
        "* The data is loaded into a **pandas DataFrame**, which allows us to easily explore, clean, and analyze the dataset.\n",
        "* The resulting DataFrame is stored in the variable `df` for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UdIFL_pwu9dY"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAbi-P2yvnwT"
      },
      "source": [
        "**Preview first rows**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az3zsNPqy5g7"
      },
      "source": [
        "Displays the first few rows of the DataFrame to quickly inspect the structure and sample values of the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "L-tArExSvCfF",
        "outputId": "d85903e7-2b2b-4564-b437-bde29600a58f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>region</th>\n",
              "      <th>price</th>\n",
              "      <th>year</th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>model</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>fuel</th>\n",
              "      <th>odometer</th>\n",
              "      <th>title_status</th>\n",
              "      <th>transmission</th>\n",
              "      <th>drive</th>\n",
              "      <th>type</th>\n",
              "      <th>paint_color</th>\n",
              "      <th>description</th>\n",
              "      <th>state</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>posting_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7316853962</td>\n",
              "      <td>fresno / madera</td>\n",
              "      <td>19999</td>\n",
              "      <td>2012</td>\n",
              "      <td>chevrolet</td>\n",
              "      <td>silverado 1500</td>\n",
              "      <td>8 cylinders</td>\n",
              "      <td>gas</td>\n",
              "      <td>141020</td>\n",
              "      <td>clean</td>\n",
              "      <td>automatic</td>\n",
              "      <td>4wd</td>\n",
              "      <td>truck</td>\n",
              "      <td>black</td>\n",
              "      <td>2012 Chevrolet Silverado 1500 LT Pickup 4D 6 1...</td>\n",
              "      <td>ca</td>\n",
              "      <td>36.790784</td>\n",
              "      <td>-119.789897</td>\n",
              "      <td>04-05-2021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7316834478</td>\n",
              "      <td>fresno / madera</td>\n",
              "      <td>19999</td>\n",
              "      <td>2010</td>\n",
              "      <td>chevrolet</td>\n",
              "      <td>silverado 1500</td>\n",
              "      <td>8 cylinders</td>\n",
              "      <td>gas</td>\n",
              "      <td>140616</td>\n",
              "      <td>clean</td>\n",
              "      <td>automatic</td>\n",
              "      <td>rwd</td>\n",
              "      <td>truck</td>\n",
              "      <td>silver</td>\n",
              "      <td>2010 Chevrolet Silverado 1500 LTZ Pickup 4D 5 ...</td>\n",
              "      <td>ca</td>\n",
              "      <td>36.774617</td>\n",
              "      <td>-119.791164</td>\n",
              "      <td>04-05-2021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7316774813</td>\n",
              "      <td>fresno / madera</td>\n",
              "      <td>23995</td>\n",
              "      <td>2013</td>\n",
              "      <td>ford</td>\n",
              "      <td>f-150</td>\n",
              "      <td>8 cylinders</td>\n",
              "      <td>gas</td>\n",
              "      <td>98607</td>\n",
              "      <td>clean</td>\n",
              "      <td>automatic</td>\n",
              "      <td>4wd</td>\n",
              "      <td>pickup</td>\n",
              "      <td>white</td>\n",
              "      <td>Economy Auto Sales 3049 Atchison St, Riverbank...</td>\n",
              "      <td>ca</td>\n",
              "      <td>37.738239</td>\n",
              "      <td>-120.939159</td>\n",
              "      <td>04-05-2021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7316749411</td>\n",
              "      <td>fresno / madera</td>\n",
              "      <td>10700</td>\n",
              "      <td>2011</td>\n",
              "      <td>honda</td>\n",
              "      <td>civic</td>\n",
              "      <td>4 cylinders</td>\n",
              "      <td>gas</td>\n",
              "      <td>171457</td>\n",
              "      <td>clean</td>\n",
              "      <td>automatic</td>\n",
              "      <td>fwd</td>\n",
              "      <td>coupe</td>\n",
              "      <td>silver</td>\n",
              "      <td>2011 Honda Civic EX Coupe! Silver, automatic, ...</td>\n",
              "      <td>ca</td>\n",
              "      <td>36.769967</td>\n",
              "      <td>-119.700165</td>\n",
              "      <td>04-05-2021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7316585031</td>\n",
              "      <td>fresno / madera</td>\n",
              "      <td>19000</td>\n",
              "      <td>2008</td>\n",
              "      <td>jeep</td>\n",
              "      <td>wrangler</td>\n",
              "      <td>6 cylinders</td>\n",
              "      <td>gas</td>\n",
              "      <td>146000</td>\n",
              "      <td>clean</td>\n",
              "      <td>automatic</td>\n",
              "      <td>4wd</td>\n",
              "      <td>SUV</td>\n",
              "      <td>black</td>\n",
              "      <td>Jeep Sahara Wrangler 2008. 146k miles 4X4. Ver...</td>\n",
              "      <td>ca</td>\n",
              "      <td>36.641000</td>\n",
              "      <td>-119.905000</td>\n",
              "      <td>03-05-2021</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id           region  price  year manufacturer           model  \\\n",
              "0  7316853962  fresno / madera  19999  2012    chevrolet  silverado 1500   \n",
              "1  7316834478  fresno / madera  19999  2010    chevrolet  silverado 1500   \n",
              "2  7316774813  fresno / madera  23995  2013         ford           f-150   \n",
              "3  7316749411  fresno / madera  10700  2011        honda           civic   \n",
              "4  7316585031  fresno / madera  19000  2008         jeep        wrangler   \n",
              "\n",
              "     cylinders fuel  odometer title_status transmission drive    type  \\\n",
              "0  8 cylinders  gas    141020        clean    automatic   4wd   truck   \n",
              "1  8 cylinders  gas    140616        clean    automatic   rwd   truck   \n",
              "2  8 cylinders  gas     98607        clean    automatic   4wd  pickup   \n",
              "3  4 cylinders  gas    171457        clean    automatic   fwd   coupe   \n",
              "4  6 cylinders  gas    146000        clean    automatic   4wd     SUV   \n",
              "\n",
              "  paint_color                                        description state  \\\n",
              "0       black  2012 Chevrolet Silverado 1500 LT Pickup 4D 6 1...    ca   \n",
              "1      silver  2010 Chevrolet Silverado 1500 LTZ Pickup 4D 5 ...    ca   \n",
              "2       white  Economy Auto Sales 3049 Atchison St, Riverbank...    ca   \n",
              "3      silver  2011 Honda Civic EX Coupe! Silver, automatic, ...    ca   \n",
              "4       black  Jeep Sahara Wrangler 2008. 146k miles 4X4. Ver...    ca   \n",
              "\n",
              "         lat        long posting_date  \n",
              "0  36.790784 -119.789897   04-05-2021  \n",
              "1  36.774617 -119.791164   04-05-2021  \n",
              "2  37.738239 -120.939159   04-05-2021  \n",
              "3  36.769967 -119.700165   04-05-2021  \n",
              "4  36.641000 -119.905000   03-05-2021  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-BBl5sVQYlk"
      },
      "source": [
        "Let's define the schema of the CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oiGbqiT-SJ8b"
      },
      "outputs": [],
      "source": [
        "df_schema=\"\"\"\n",
        "id: unique listing identifier,\n",
        "region: geographic listing region,\n",
        "price: listed vehicle price,\n",
        "year: model year of the vehicle,\n",
        "manufacturer: vehicle manufacturer,\n",
        "model: vehicle model name,\n",
        "cylinders: engine cylinder configuration,\n",
        "fuel: fuel type,\n",
        "odometer: vehicle mileage,\n",
        "title_status: legal title status,\n",
        "transmission: transmission type,\n",
        "drive: drivetrain type,\n",
        "type: vehicle body type,\n",
        "paint_color: exterior color,\n",
        "description: free-text listing description,\n",
        "state: state where vehicle is listed,\n",
        "lat: latitude of listing location,\n",
        "long: longitude of listing location,\n",
        "posting_date: date when the listing was posted in dd-mm-yyyy\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENVRr6dJyrnQ"
      },
      "source": [
        "## **Data Analysis Code Generation: LLM-only Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PMcqLB53ytcc"
      },
      "outputs": [],
      "source": [
        "def code_generation(data_path,df_schema,question):\n",
        "  '''\n",
        "  Generates Python analysis code from a user question and dataset schema using an LLM.\n",
        "\n",
        "  Parameters:\n",
        "  - data_path (str): Path to the CSV file.\n",
        "  - df (pandas.DataFrame): Dataset used to infer column names.\n",
        "  - question (str): Analytical question to answer.\n",
        "\n",
        "  Returns:\n",
        "  - str: LLM-generated Python code only.\n",
        "  '''\n",
        "\n",
        "  code_generation_prompt = f\"\"\"\n",
        "  ###ROLE\n",
        "  You are a data analyst with 10+ years of experience in the retail car business.\n",
        "\n",
        "  ###OBJECTIVE\n",
        "  Based on the user question and the data schema provided, you need to write the Python code to perform the relevant analysis to answer the question.\n",
        "\n",
        "  ###INPUT\n",
        "  CSV file name: {data_path}\n",
        "  CSV schema: {df_schema}\n",
        "  Question: {question}\n",
        "\n",
        "  ###OUTPUT\n",
        "  The output should ONLY be the Python code. Make some major inefficiencies in the Python code. This will be used for educational purposes to teach students how to write code.\n",
        "  No other explanation or reasoning is required.\n",
        "  \"\"\"\n",
        "\n",
        "  response_from_llm = llm.invoke(code_generation_prompt)\n",
        "\n",
        "  return response_from_llm.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3BtTrlcRUHNT"
      },
      "outputs": [],
      "source": [
        "def code_evaluation(question,code,df_schema):\n",
        "\n",
        "  '''\n",
        "  Evaluates whether generated Python code correctly answers a given analytical question.\n",
        "\n",
        "  The function prompts an LLM to strictly assess the code against the question\n",
        "  and the provided dataframe schema, ensuring correctness, completeness, and schema compliance.\n",
        "\n",
        "  Parameters:\n",
        "  - question (str): The analytical question being evaluated.\n",
        "  - code (str): Generated Python code to review.\n",
        "  - df_columns (list): List of valid dataframe column names (single source of truth).\n",
        "\n",
        "  Returns:\n",
        "  - str: JSON-formatted evaluation containing a numeric rating (1–10) and a concise rationale.\n",
        "  '''\n",
        "\n",
        "  evaluation_prompt = f\"\"\"\n",
        "  ### ROLE\n",
        "  You're an expert Python code tester.\n",
        "\n",
        "  ### OBJECTIVE\n",
        "  Given a question, the associated data schema to use for answering it, and the Python code to answer the question, you are required to evaluate the accuracy and completeness of the code.\n",
        "\n",
        "  ### INPUT\n",
        "  Question: {question}\n",
        "  Schema: {df_schema}\n",
        "  Code: {code}\n",
        "\n",
        "  ### SCORING SCHEME\n",
        "  10: Fully correct\n",
        "  7–9: Minor gaps owing to missed filtering or aggregation or incorrect variable names\n",
        "  4–6: Incomplete code owing to missing key logic(s)\n",
        "  1–3: Incorrect code owing to schema violations and/or syntactical errors\n",
        "\n",
        "  ### OUTPUT\n",
        "  The output should ONLY be a dictionary with the following keys:\n",
        "\n",
        "  {{\n",
        "      \"score\": \"between_1_and_10\",\n",
        "      \"rationale\": \"One concise sentence summarizing the rationale for the score\"\n",
        "  }}\n",
        "  Do NOT fix, rewrite, execute, or explain the code.\n",
        "  \"\"\"\n",
        "\n",
        "  response_from_llm = evaluator_llm.invoke(evaluation_prompt)\n",
        "\n",
        "  return response_from_llm.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOAtqOw7GNwq"
      },
      "source": [
        "## **Test Cases: LLM-only Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm5_OhFsIEqT"
      },
      "source": [
        "To evaluate how well a single-prompt LLM approach handles increasingly complex analytical questions, we test it across three levels of rigor:\n",
        "\n",
        "- **Rigour Level 1:** How does the average listing price vary across different manufacturers for vehicles manufactured after 2015?\n",
        "- **Rigour Level 2:** Which regions have the highest average listing prices for vehicles with a clean title status?\n",
        "- **Rigour Level 3:** Using only vehicles with model year 2017, compute for each state:\n",
        "  - the average price of automatic-transmission vehicles\n",
        "  - the overall average price of all vehicles in that state\n",
        "  \n",
        "  Calculate the percentage deviation of automatic vehicles from the state average and return the single state with the maximum absolute deviation, along with the deviation value.\n",
        "\n",
        "As we move toward higher rigor, the lack of structure and validation becomes more apparent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9fnhyiD_vwb"
      },
      "source": [
        "### Rigour Level 1: How does the average listing price vary across different manufacturers for vehicles manufactured after 2015?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OD4nV0tYmpa0"
      },
      "outputs": [],
      "source": [
        "question_1=\"\"\"How does the average listing price vary across different manufacturers for vehicles manufactured after 2015?\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gazGXUFL3CVE"
      },
      "outputs": [],
      "source": [
        "code_1=code_generation(data_path,df_schema,question_1)\n",
        "evaluation_1=code_evaluation(question_1,code_1,df_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WzOcbVxshwp"
      },
      "source": [
        "Here’s the code that was generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0SpZfGosmYb",
        "outputId": "3dfe12b2-8b3d-439d-bca6-5255eb7eb3e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "# Load the dataset\n",
            "data = pd.read_csv('used_car_dataset.csv')\n",
            "\n",
            "# Filter for vehicles manufactured after 2015\n",
            "filtered_data = data[data['year'] > 2015]\n",
            "\n",
            "# Create an empty dictionary to hold manufacturer prices\n",
            "manufacturer_prices = {}\n",
            "\n",
            "# Loop through each row in the filtered data\n",
            "for index, row in filtered_data.iterrows():\n",
            "    manufacturer = row['manufacturer']\n",
            "    price = row['price']\n",
            "    \n",
            "    # If manufacturer is not in the dictionary, add it with an empty list\n",
            "    if manufacturer not in manufacturer_prices:\n",
            "        manufacturer_prices[manufacturer] = []\n",
            "    \n",
            "    # Append the price to the manufacturer's list\n",
            "    manufacturer_prices[manufacturer].append(price)\n",
            "\n",
            "# Calculate average prices for each manufacturer\n",
            "average_prices = {}\n",
            "for manufacturer, prices in manufacturer_prices.items():\n",
            "    total_price = 0\n",
            "    for price in prices:\n",
            "        total_price += price\n",
            "    average_price = total_price / len(prices)\n",
            "    average_prices[manufacturer] = average_price\n",
            "\n",
            "# Convert the average prices dictionary to a DataFrame for better visualization\n",
            "average_prices_df = pd.DataFrame(list(average_prices.items()), columns=['Manufacturer', 'Average Price'])\n",
            "\n",
            "# Print the average prices DataFrame\n",
            "print(average_prices_df)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(code_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mikjy9I0sqL6"
      },
      "source": [
        "Check the **score** assigned to the code and the corresponding **rationale**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XPNkG1OZt-gz"
      },
      "outputs": [],
      "source": [
        "def extract_json(llm_output):\n",
        "  json_text = re.search(r\"\\{.*\\}\", llm_output, re.DOTALL).group()\n",
        "  result = json.loads(json_text)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEWS_kepvyf5",
        "outputId": "e30f1f59-9dd7-4045-837b-45f66b065383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The score of the code is 10\n",
            "\n",
            "\n",
            "The rationale for the score provided is: The code correctly filters vehicles manufactured after 2015, calculates the average listing price for each manufacturer, and outputs the results in a DataFrame.\n"
          ]
        }
      ],
      "source": [
        "llm_result_1 = extract_json(evaluation_1)\n",
        "\n",
        "print(f\"The score of the code is {llm_result_1['score']}\")\n",
        "print(\"\\n\")\n",
        "print(f\"The rationale for the score provided is: {llm_result_1['rationale']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ovLu-tE3RRN"
      },
      "source": [
        "Let's execute the code provided by the LLM and check if we get the desired results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjerk21ep-yu",
        "outputId": "7d189615-ffa2-49ae-e2cd-e07fdd05691e"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/used_car_dataset.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/content/used_car_dataset.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Filter for vehicles manufactured after 2015\u001b[39;00m\n\u001b[32m      7\u001b[39m filtered_df = df[df[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m] > \u001b[32m2015\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/used_car_dataset.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/used_car_dataset.csv')\n",
        "\n",
        "# Filter for vehicles manufactured after 2015\n",
        "filtered_df = df[df['year'] > 2015]\n",
        "\n",
        "# Calculate the average listing price by manufacturer\n",
        "average_price_by_manufacturer = filtered_df.groupby('manufacturer')['price'].mean().reset_index()\n",
        "\n",
        "# Sort the results by average price\n",
        "average_price_by_manufacturer = average_price_by_manufacturer.sort_values(by='price', ascending=False)\n",
        "\n",
        "# Display the result\n",
        "print(average_price_by_manufacturer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aODwKFAF3bKe"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "The LLM correctly generated code to filter vehicles manufactured after 2015 and compute manufacturer-wise average listing prices for this straightforward aggregation task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5dsrgOg_yHb"
      },
      "source": [
        "### Rigour Level 2: Which regions have the highest average listing prices for vehicles with a clean title status?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "voELTyTXxDpw"
      },
      "outputs": [],
      "source": [
        "question_2=\"Which regions have the highest average listing prices for vehicles with a clean title status?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-QPLnev33Eo2"
      },
      "outputs": [],
      "source": [
        "code_2=code_generation(data_path,df_schema,question_2)\n",
        "evaluation_2=code_evaluation(question_2,code_2,df_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaRjOzbqxRG-"
      },
      "source": [
        "Here’s the code that was generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLSgEx9UxRG_",
        "outputId": "e4b5ce55-f367-4c9d-ab2b-59d3acd0b14e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "# Load the dataset\n",
            "data = pd.read_csv('used_car_dataset.csv')\n",
            "\n",
            "# Filter the dataset for clean title status\n",
            "clean_title_data = data[data['title_status'] == 'clean']\n",
            "\n",
            "# Create an empty list to store regions and their average prices\n",
            "region_avg_prices = []\n",
            "\n",
            "# Loop through each unique region\n",
            "for region in clean_title_data['region'].unique():\n",
            "    # Filter data for the current region\n",
            "    region_data = clean_title_data[clean_title_data['region'] == region]\n",
            "    \n",
            "    # Calculate the average price for the current region\n",
            "    avg_price = region_data['price'].mean()\n",
            "    \n",
            "    # Append the region and average price to the list\n",
            "    region_avg_prices.append((region, avg_price))\n",
            "\n",
            "# Convert the list to a DataFrame\n",
            "avg_price_df = pd.DataFrame(region_avg_prices, columns=['region', 'average_price'])\n",
            "\n",
            "# Sort the DataFrame by average price in descending order\n",
            "sorted_avg_price_df = avg_price_df.sort_values(by='average_price', ascending=False)\n",
            "\n",
            "# Print the sorted DataFrame\n",
            "print(sorted_avg_price_df)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(code_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_ORantixRG_"
      },
      "source": [
        "Check the **score** assigned to the code and the corresponding **rationale**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3beFzBo-xRG_",
        "outputId": "4bb02ca6-2018-40a1-b300-d178ed15c5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The score of the code is 10\n",
            "\n",
            "\n",
            "The rationale for the score provided is: The code correctly filters for clean title status, calculates average prices per region, and sorts the results as required by the question.\n"
          ]
        }
      ],
      "source": [
        "llm_result_2 = extract_json(evaluation_2)\n",
        "\n",
        "print(f\"The score of the code is {llm_result_2['score']}\")\n",
        "print(\"\\n\")\n",
        "print(f\"The rationale for the score provided is: {llm_result_2['rationale']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2tyeZ7QxRHA"
      },
      "source": [
        "Let's execute the code provided by the LLM and check if we get the desired results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLzoYAgZKm5t",
        "outputId": "13d4866c-e36f-4cfb-b814-4b29b913d09e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     region         price\n",
            "0                      bend  20565.021538\n",
            "9   spokane / coeur d'alene  20557.301724\n",
            "6             new hampshire  18718.047273\n",
            "7    omaha / council bluffs  18680.107438\n",
            "3           fresno / madera  17932.576744\n",
            "1                  columbus  16788.815217\n",
            "8                   orlando  16557.158730\n",
            "5  kennewick-pasco-richland  14869.020833\n",
            "2                    eugene  14056.226316\n",
            "4              jacksonville  13959.894977\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('used_car_dataset.csv')\n",
        "\n",
        "# Filter for clean title status\n",
        "clean_title_data = data[data['title_status'] == 'clean']\n",
        "\n",
        "# Calculate the average listing price by region\n",
        "average_price_by_region = clean_title_data.groupby('region')['price'].mean().reset_index()\n",
        "\n",
        "# Sort the results by average price in descending order\n",
        "highest_average_price_regions = average_price_by_region.sort_values(by='price', ascending=False)\n",
        "\n",
        "# Display the result\n",
        "print(highest_average_price_regions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUGHfdyEUlBr"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "The LLM received a high score as the generated code correctly filtered clean-title vehicles, aggregated prices by region, and sorted results to identify regions with the highest average listing prices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD7ZNgYp_z5S"
      },
      "source": [
        "### Rigour Level 3: Using only vehicles with model year 2017, compute for each state:\n",
        "  - the average price of automatic-transmission vehicles\n",
        "  - the overall average price of all vehicles in that state\n",
        "  \n",
        "  Calculate the percentage deviation of automatic vehicles from the state average and return the single state with the maximum absolute deviation, along with the deviation value.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "l1vCAhKGxmqs"
      },
      "outputs": [],
      "source": [
        "question_3=\"\"\"Using only vehicles with model year 2017, compute for each state:\n",
        "  - the average price of automatic-transmission vehicles\n",
        "  - the overall average price of all vehicles in that state\n",
        "\n",
        "  Calculate the percentage deviation of automatic vehicles from the state average and return the single state with the maximum absolute deviation, along with the deviation value.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6dwdThk53Fjh"
      },
      "outputs": [],
      "source": [
        "code_3=code_generation(data_path,df_schema,question_3)\n",
        "evaluation_3=code_evaluation(question_3,code_3,df_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTWl9iX-x3m9"
      },
      "source": [
        "Here’s the code that was generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BepWDD4nx3m-",
        "outputId": "db31cf4f-aa99-484b-88b8-23a8b917cf2b"
      },
      "outputs": [],
      "source": [
        "print(code_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHOlVWwIx3m-"
      },
      "source": [
        "Check the **score** assigned to the code and the corresponding **rationale**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX8J2MdRx3m-",
        "outputId": "f9cc574c-9d61-45f5-86a5-15cb0d797dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The score of the code is 10\n",
            "\n",
            "\n",
            "The rationale for the score provided is: The code correctly filters, calculates averages, computes deviations, and identifies the state with the maximum absolute deviation as per the given requirements.\n"
          ]
        }
      ],
      "source": [
        "llm_result_3 = extract_json(evaluation_3)\n",
        "\n",
        "print(f\"The score of the code is {llm_result_3['score']}\")\n",
        "print(\"\\n\")\n",
        "print(f\"The rationale for the score provided is: {llm_result_3['rationale']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLYRuPwIUwY_"
      },
      "source": [
        "Let's execute the code provided by the LLM and check if we get the desired results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmI8IXgtKvFW",
        "outputId": "964440fd-561e-4a53-a4f8-bd870c0370f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('wa', np.float64(-2.716793753401363))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('used_car_dataset.csv')\n",
        "\n",
        "# Filter for model year 2017\n",
        "df_2017 = df[df['year'] == 2017]\n",
        "\n",
        "# Calculate average price of automatic vehicles and overall average price by state\n",
        "state_avg = df_2017.groupby('state')['price'].mean().reset_index(name='overall_avg_price')\n",
        "auto_avg = df_2017[df_2017['transmission'] == 'automatic'].groupby('state')['price'].mean().reset_index(name='auto_avg_price')\n",
        "\n",
        "# Merge the two averages\n",
        "merged_avg = pd.merge(state_avg, auto_avg, on='state', how='left')\n",
        "\n",
        "# Calculate percentage deviation\n",
        "merged_avg['deviation'] = (merged_avg['auto_avg_price'] - merged_avg['overall_avg_price']) / merged_avg['overall_avg_price'] * 100\n",
        "\n",
        "# Find the state with the maximum absolute deviation\n",
        "max_deviation_state = merged_avg.loc[merged_avg['deviation'].abs().idxmax()]\n",
        "\n",
        "# Output the result\n",
        "result = (max_deviation_state['state'], max_deviation_state['deviation'])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoDhmO7nU0kn"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "The LLM received a high evaluation score as the generated code correctly restricted data to model year 2017, computed both automatic-only and overall state-wise average prices, calculated percentage deviation, and accurately identified the state with the maximum absolute deviation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QehZ2YZsH3sv"
      },
      "source": [
        "### Observations: LLM Code Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOtVuy-kHzR8"
      },
      "source": [
        "* The **LLM is able to answer analytical questions correctly** and generate working code across tasks.\n",
        "\n",
        "* However, it can **hallucinate**, offers **no explicit control over its reasoning**, and **cannot automatically recover or regenerate outputs** when failures occur.\n",
        "\n",
        "* At an **organizational scale**, this lack of control makes standalone LLM-based systems **difficult to govern, audit, and extend reliably**.\n",
        "\n",
        "* Capabilities such as **code execution, result inspection, observation collection, and iterative correction** must be added externally, as they are **not natively supported by the LLM**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cun6YyRX3Ic5"
      },
      "source": [
        "## **Data Analysis Code Generation: Agentic AI Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzWKvLu0IYwS"
      },
      "source": [
        "To address these limitations, we move from direct prompt-based code generation to an **agentic, node-based workflow.**\n",
        "\n",
        "Instead of asking the LLM to immediately produce code, we decompose the task into multiple well-defined steps:\n",
        "\n",
        "- Planner Node : Interpret the question, resolve ambiguity, select relevant columns, and define all required calculations explicitly.\n",
        "\n",
        "- Validation Node: Check the plan for schema adherence, logical completeness, and alignment with the question.\n",
        "\n",
        "- Replanner Node : Refine or fix the plan if gaps, inconsistencies, or missing steps are detected.\n",
        "\n",
        "- Code Generation Node: Convert the validated plan into deterministic, executable code and generate the final output.\n",
        "\n",
        "- Code Evaluation Node: Assess the generated code against the original question and schema to ensure correctness, completeness, and logical fidelity.\n",
        "\n",
        "Each step is handled by a dedicated node, making the system more **robust, explainable, and reliable**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLztDD-Q3rSf"
      },
      "source": [
        "### Define Agent State"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7VfS_sFIlAz"
      },
      "source": [
        "All nodes in the workflow communicate through a shared state object.  \n",
        "This ensures that:\n",
        "\n",
        "- Information flows **consistently** across nodes\n",
        "- Intermediate outputs (plans, decisions, code) are **explicitly stored**\n",
        "- The system remains **traceable and debuggable**\n",
        "\n",
        "This design is critical for building reliable multi-step analytical pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5h_GJil3K5G"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    csv_schema: str\n",
        "    csv_paths: dict\n",
        "    plan: str\n",
        "    user_approved: bool\n",
        "    code: str\n",
        "    reasoning: str\n",
        "    replan_count: int\n",
        "    evaluation: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDT9ovIZ3wUu"
      },
      "source": [
        "### Define Agent Nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zmkmzxzIsg3"
      },
      "source": [
        "Rather than relying on a single monolithic LLM call, we break the analysis into specialized nodes, each responsible for a specific role.\n",
        "\n",
        "This separation of concerns:\n",
        "- Improves reasoning quality\n",
        "- Enables validation and correction\n",
        "- Reduces hallucinations\n",
        "- Increases trust in the final output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpFhnRwg6Epu"
      },
      "source": [
        "#### Planning Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJAmdjPD6HDV"
      },
      "source": [
        "Define the Planning Agent to reason about the analytical steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Njroj47WHSq"
      },
      "outputs": [],
      "source": [
        "def planner(state: AgentState):\n",
        "\n",
        "    '''\n",
        "    Generates a step-by-step analytical plan to answer a user question using a given CSV schema.\n",
        "\n",
        "    The function prompts an LLM to decompose the user’s question into clear, computation-ready\n",
        "    analytical steps while strictly adhering to the provided schema as the single source of truth.\n",
        "\n",
        "    Parameters:\n",
        "    - state (AgentState): Dictionary-like object containing the user question and CSV schema.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary with a single key, 'plan', containing the generated analytical plan.\n",
        "    '''\n",
        "\n",
        "    planning_prompt = f\"\"\"\n",
        "\n",
        "### ROLE\n",
        "You are a senior data analyst with 10+ years of experience in the retail automobile industry.\n",
        "Your task is to produce a detailed, computation-ready coding plan with strict schema discipline.\n",
        "\n",
        "### INPUT\n",
        "User Question:\n",
        "{state['question']}\n",
        "\n",
        "Available CSV schema (single source of truth):\n",
        "{state['csv_schema']}\n",
        "\n",
        "### OBJECTIVE\n",
        "\n",
        "Break the question into multiple parts based on the ask and then create a step-by-step plan for each part of the question.\n",
        "\n",
        "Create a complete analytical plan that fully answers the question using only the provided schema, clearly defining all metrics,\n",
        "calculations, and comparisons, and structuring the steps so that the next agent can directly translate the plan into Python code without any additional interpretation.\n",
        "\n",
        "\n",
        "### OUTPUT FORMAT\n",
        "- Provide step-by-step instructions to complete the task.\n",
        "- Do not include code in the output, it should strictly be just a plan.\n",
        "\n",
        "\"\"\"\n",
        "    plan = llm.invoke(planning_prompt).content.strip()\n",
        "    return {\"plan\": plan}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7kgFaW1FV-P"
      },
      "source": [
        "#### Plan Validator Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lArIzsoRFXlp"
      },
      "source": [
        "Evaluate the quality and correctness of the proposed plan and approve or reject it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpUU6aeTBJvO"
      },
      "outputs": [],
      "source": [
        "def plan_validator(state: AgentState):\n",
        "\n",
        "    '''\n",
        "    Validates an analysis plan against the user question and CSV schema for correctness and completeness.\n",
        "\n",
        "    Parameters:\n",
        "    - state (AgentState): Contains the question, CSV schema, and proposed plan.\n",
        "\n",
        "    Returns:\n",
        "    - dict: {'user_approved': True} if approved, else {'user_approved': False}.\n",
        "    '''\n",
        "\n",
        "    validation_prompt = f\"\"\"\n",
        "### ROLE\n",
        "You are a validation agent responsible for evaluating the quality and correctness of an analysis plan.\n",
        "\n",
        "### INPUT\n",
        "User Question:\n",
        "{state['question']}\n",
        "\n",
        "Available CSV schema (single source of truth):\n",
        "{state['csv_schema']}\n",
        "\n",
        "Proposed Analysis Plan:\n",
        "{state['plan']}\n",
        "\n",
        "### OBJECTIVE\n",
        "Validate whether the proposed analysis plan correctly and completely answers the user question using only the provided CSV schema.\n",
        "\n",
        "### VALIDATION CRITERIA\n",
        "1. The plan correctly captures the analytical intent of the user question.\n",
        "2. All steps rely strictly on the available schema without assumed, inferred, or external data.\n",
        "3. The steps are logically ordered and collectively sufficient to answer the question.\n",
        "4. All conditions implied by the question (filters, groupings, aggregations, comparisons, ordering) are explicitly included.\n",
        "5. Each step is clear, unambiguous, and practically executable.\n",
        "\n",
        "### OUTPUT\n",
        "- The output should be ONLY one word out of APPROVED or REJECTED, depending on whether the validation criterion are satisfied or violated.\n",
        "- Do not include any additional explanations, comments, or text.\n",
        "\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "    decision = evaluator_llm.invoke(validation_prompt).content.strip()\n",
        "\n",
        "    if decision == \"APPROVED\":\n",
        "        return {\"user_approved\": True}\n",
        "    else:\n",
        "        return {\"user_approved\": False}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6vYb5xJFaEW"
      },
      "source": [
        "#### Replanner Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfgHt4MsFb0T"
      },
      "source": [
        "Generate an improved plan if the original plan is rejected, retrying up to a maximum number of attempts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cQ2v73FBvo0"
      },
      "outputs": [],
      "source": [
        "def replanner(state: AgentState):\n",
        "\n",
        "    '''\n",
        "    Regenerates a corrected analysis plan when a previous plan is rejected.\n",
        "\n",
        "    Increments a replanning counter, enforces a maximum retry limit, and prompts an LLM\n",
        "    to produce an improved, schema-compliant analytical plan.\n",
        "\n",
        "    Parameters:\n",
        "    - state (AgentState): Contains the question, CSV schema, rejected plan,\n",
        "      and replanning metadata.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Updated state with a revised 'plan' and 'user_approved' set to False.\n",
        "\n",
        "    Raises:\n",
        "    - Exception: If the maximum number of replanning attempts is exceeded.\n",
        "    '''\n",
        "\n",
        "\n",
        "     # Increment replan count\n",
        "    old_count=state['replan_count']\n",
        "    state[\"replan_count\"] = old_count + 1\n",
        "\n",
        "    # Check if max replans reached\n",
        "    if state[\"replan_count\"] > 3:\n",
        "        raise Exception(\"Maximum replanning attempts reached. Unable to generate a valid plan.\")\n",
        "\n",
        "    replanner_prompt = f\"\"\"\n",
        "### ROLE\n",
        "You are a data analysis reasoning agent responsible for producing a corrected and improved analysis plan after a previous plan was rejected.\n",
        "\n",
        "### INPUT\n",
        "User Question:\n",
        "{state['question']}\n",
        "\n",
        "Available CSV schema (single source of truth):\n",
        "{state['csv_schema']}\n",
        "\n",
        "Previous Rejected Plan:\n",
        "{state['plan']}\n",
        "\n",
        "### OBJECTIVE\n",
        "Generate a correct, complete, and logically sound analysis plan that answers the user question using only the provided CSV schema.\n",
        "\n",
        "### INSTRUCTIONS\n",
        "1. Carefully analyze the user question and the available schema.\n",
        "2. Identify and correct any logical errors, omissions, or ambiguities in the previous plan.\n",
        "3. Ensure every step is feasible using only the given columns; do not assume or invent data.\n",
        "4. Explicitly include all required operations such as filtering, grouping, aggregation, comparison, and ordering as implied by the question.\n",
        "5. Produce a clear, logically ordered, numbered list of steps in plain language.\n",
        "\n",
        "### OUTPUT\n",
        "Return ONLY the improved analysis plan as a numbered list, formatted as plain text, suitable for the next code generation step.\n",
        "\"\"\"\n",
        "\n",
        "    new_plan = llm.invoke(replanner_prompt).content.strip()\n",
        "    return {\"plan\": new_plan, \"user_approved\": False}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9seLwZI2xM"
      },
      "source": [
        "The planning phase is intentionally isolated from code generation.\n",
        "\n",
        "Key advantages:\n",
        "- The **Planner Node** focuses purely on analytical reasoning.\n",
        "- The **Validator Node** enforces schema correctness and logical completeness.\n",
        "- The **Replanner Node** provides a controlled recovery mechanism when plans fail validation.\n",
        "\n",
        "This loop ensures that only **sound and feasible plans** move forward.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kizft_mFd6g"
      },
      "source": [
        "#### Plan-to-Code Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xn7dHwaI6Pf"
      },
      "source": [
        "Only after a plan is approved do we convert it into executable code.\n",
        "\n",
        "This separation ensures:\n",
        "- Code strictly follows validated logic\n",
        "- No hidden assumptions are introduced\n",
        "- Errors are minimized at execution time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRwIJU-bFg2x"
      },
      "source": [
        "Convert the approved analytical plan into executable Python code using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfNw4nntCP1T"
      },
      "outputs": [],
      "source": [
        "def plan_to_code(state: AgentState):\n",
        "\n",
        "    '''\n",
        "    Converts an approved analysis plan into executable pandas code.\n",
        "\n",
        "    Parameters:\n",
        "    - state (AgentState): Contains the user question, CSV paths, schema,\n",
        "      and the validated analysis plan.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary with key 'code' containing executable Python code.\n",
        "    '''\n",
        "\n",
        "    code_generation_prompt = f\"\"\"\n",
        "### Role\n",
        "You are a Senior Data Analyst who converts an analytical plan into accurate, error-free, executable Python code\n",
        "\n",
        "### INPUT\n",
        "User Question:\n",
        "{state['question']}\n",
        "\n",
        "CSV file paths:\n",
        "{state['csv_paths']}\n",
        "\n",
        "Available CSV schema (single source of truth):\n",
        "{state['csv_schema']}\n",
        "\n",
        "Coding analysis plan to implement:\n",
        "{state['plan']}\n",
        "\n",
        "### OBJECTIVE\n",
        "\n",
        "Based on the user question and coding analysis plan provided, you are required to convert the plan\n",
        "into accurate, error-free, executable Python code, retaining only the necessary snippets to answer the question.\n",
        "\n",
        "### OUTPUT\n",
        "Return ONLY executable Python code using pandas that correctly implements the analysis plan and print the final answer.\n",
        "Do not add any text other than code\n",
        "\"\"\"\n",
        "    code = llm.invoke(code_generation_prompt).content\n",
        "    return {\"code\": code}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEwN_q1neeaq"
      },
      "source": [
        "#### Code Evaluation Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5SnK4bNGACX"
      },
      "outputs": [],
      "source": [
        "def code_evaluator(state: AgentState):\n",
        "\n",
        "\n",
        "  '''\n",
        "    Evaluates generated pandas code for correctness and schema compliance.\n",
        "\n",
        "    Uses an LLM to assess whether the code fully answers the user question\n",
        "    using only the provided CSV schema and returns a structured evaluation.\n",
        "\n",
        "    Parameters:\n",
        "    - state (AgentState): Contains the question, CSV schema, and generated code.\n",
        "\n",
        "    Returns:\n",
        "    - AgentState: Updated state including the evaluation rationale and rating.\n",
        "  '''\n",
        "\n",
        "  evaluation_prompt=f\"\"\"\n",
        "### ROLE\n",
        "You're an expert Python code tester.\n",
        "\n",
        "### OBJECTIVE\n",
        "Given a question, the associated data schema to use for answering it, and the Python code to answer the question, you are required\n",
        "to evaluate the accuracy and completeness of the code.\n",
        "\n",
        "### INPUT\n",
        "Question: {state['question']}\n",
        "Schema: {state['csv_schema']}\n",
        "Code: {state['code']}\n",
        "\n",
        "### SCORING SCHEME\n",
        "10: Fully correct\n",
        "7–9: Minor gaps owing to missed filtering or aggregation or incorrect variable names\n",
        "4–6: Incomplete code owing to missing key logic(s)\n",
        "1–3: Incorrect code owing to schema violations and/or syntactical errors\n",
        "\n",
        "### OUTPUT\n",
        "The output should ONLY be a JSON with the following keys:\n",
        "1. \"score\": \"between_1_and_10\",\n",
        "2. \"rationale\": \"One concise sentence summarizing the rationale for the score\"\n",
        "\n",
        "Do NOT fix, rewrite, execute, or explain the code.\n",
        "\"\"\"\n",
        "  response_from_llm = evaluator_llm.invoke(evaluation_prompt)\n",
        "\n",
        "  state['evaluation'] = response_from_llm.content\n",
        "\n",
        "  return state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lzADRzFFj5N"
      },
      "source": [
        "### Build LangGraph Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMV1vUCvFlhs"
      },
      "source": [
        "Add all nodes, define entry point, set conditional edges, and compile the graph into an executable workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5AgVEd9CQuF"
      },
      "outputs": [],
      "source": [
        "graph = StateGraph(AgentState)\n",
        "\n",
        "# Add all nodes\n",
        "graph.add_node(\"planner\", planner)\n",
        "graph.add_node(\"plan_validator\", plan_validator)\n",
        "graph.add_node(\"replanner\", replanner)\n",
        "graph.add_node(\"plan_to_code\", plan_to_code)\n",
        "graph.add_node(\"code_evaluator\", code_evaluator)\n",
        "\n",
        "\n",
        "# Set entry point\n",
        "graph.set_entry_point(\"planner\")\n",
        "\n",
        "# Conditional routing after planner\n",
        "graph.add_edge(\"planner\", \"plan_validator\")\n",
        "\n",
        "# Conditional routing after plan_validator\n",
        "graph.add_conditional_edges(\n",
        "    \"plan_validator\",\n",
        "    lambda state: \"approved\" if state[\"user_approved\"] else \"replan\",\n",
        "    {\n",
        "        \"approved\": \"plan_to_code\",\n",
        "        \"replan\": \"replanner\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# After replanner, re-validate the new plan\n",
        "graph.add_edge(\"replanner\", \"plan_validator\")\n",
        "\n",
        "\n",
        "graph.add_edge(\"plan_to_code\", \"code_evaluator\")\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPVT7memJAPH"
      },
      "source": [
        "The final workflow forms a controlled, agentic pipeline:\n",
        "\n",
        "1. Interpret the question\n",
        "2. Plan the analysis\n",
        "3. Validate the plan\n",
        "4. Replan if necessary\n",
        "5. Generate executable code\n",
        "6. Evaluate the executable code\n",
        "\n",
        "This structure mirrors how a human analyst approaches complex data problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XIH6qLHFnwc"
      },
      "source": [
        "#### Visualize Workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzN7ZLU1FpvD"
      },
      "source": [
        "Display a visual representation of the node-based workflow using a Mermaid diagram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIZ5JHtNbKun"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "5bLEIE8CbHVP",
        "outputId": "52856a81-880f-4b93-db31-905ef480fd4e"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRUJfuGrFw-Z"
      },
      "source": [
        "## **Test Cases: Agentic AI Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOvbht02JIg1"
      },
      "source": [
        "We now test the same three analytical queries using the **agentic system** to observe how multiple tools **plan the analysis, validate the approach, and generate the final code** in a structured and controlled manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS88hv8Uyarr"
      },
      "source": [
        "### Rigour 1: How does the average listing price vary across different manufacturers for vehicles manufactured after 2015?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHG2VmsLF5rJ"
      },
      "outputs": [],
      "source": [
        "output_1 = app.invoke({\n",
        "    \"question\": question_1,\n",
        "    \"csv_schema\": df_schema,\n",
        "    \"csv_paths\": data_path,\n",
        "     \"replan_count\" : 0,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN5I24CYyn_k"
      },
      "source": [
        "First, let’s review the plan generated by the agent to build the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNMwbo-qRhhA",
        "outputId": "d7c1a808-af4b-4c3d-b841-a1f8e13e8c6a"
      },
      "outputs": [],
      "source": [
        "print(output_1['plan'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSsvx720y3TW"
      },
      "source": [
        "Here’s the code that was generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRIs7glpprft",
        "outputId": "78ae087c-b539-4438-efd9-694d8443bc02"
      },
      "outputs": [],
      "source": [
        "print(output_1['code'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clmRMeXHy8-b"
      },
      "source": [
        "Check the **score** assigned to the code and the corresponding **rationale**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsSNEHRdy9eJ",
        "outputId": "ff3012f7-66b6-40b1-e223-e8c9f3b3d295"
      },
      "outputs": [],
      "source": [
        "agent_result_1 = output_1['evaluation']\n",
        "agent_result_1 = extract_json(agent_result_1)\n",
        "print(f\"The score of the code is {agent_result_1['score']}\")\n",
        "print(\"\\n\")\n",
        "print(f\"The rationale for the score provided is: {agent_result_1['rationale']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0JmFU9LoQdw"
      },
      "source": [
        "Let's execute the code provided by the Agent and check if we get the desired results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2f3oq22H05X",
        "outputId": "4df87bd7-33ed-4503-9ff4-40f05482d8d4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Data\n",
        "df = pd.read_csv('used_car_dataset.csv')\n",
        "\n",
        "# Step 2: Filter the Data\n",
        "filtered_df = df[df['year'] > 2015]\n",
        "\n",
        "# Step 3: Select Relevant Columns\n",
        "relevant_df = filtered_df[['manufacturer', 'price']]\n",
        "\n",
        "# Step 4: Group Data by Manufacturer\n",
        "grouped_df = relevant_df.groupby('manufacturer')\n",
        "\n",
        "# Step 5: Calculate Average Price\n",
        "average_price_df = grouped_df['price'].mean().reset_index()\n",
        "\n",
        "# Step 6: Create a Summary DataFrame\n",
        "average_price_df.columns = ['manufacturer', 'average_price']\n",
        "\n",
        "# Step 7: Sort the Results\n",
        "sorted_average_price_df = average_price_df.sort_values(by='average_price', ascending=False)\n",
        "\n",
        "# Step 8: Prepare for Output\n",
        "sorted_average_price_df['average_price'] = sorted_average_price_df['average_price'].round(2)\n",
        "\n",
        "# Final Output\n",
        "print(sorted_average_price_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfrbA5Huog72"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "The agents produced a clear step-by-step plan, validated the approach, and generated correct code, demonstrating structured reasoning even for a simple analytical query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utkh6_1Tyars"
      },
      "source": [
        "### Rigour 2: Which regions have the highest average listing prices for vehicles with a clean title status?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnLISx_tF5rK"
      },
      "outputs": [],
      "source": [
        "output_2 = app.invoke({\n",
        "    \"question\": question_2,\n",
        "    \"csv_schema\": df_schema,\n",
        "    \"csv_paths\": data_path,\n",
        "    \"replan_count\" : 0,\n",
        "\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbSmDDln12g9"
      },
      "source": [
        "First, let’s review the plan generated by the agent to build the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMDBXtlD12g9",
        "outputId": "31f6ee2d-ca83-432b-c734-b746c6c62c15"
      },
      "outputs": [],
      "source": [
        "print(output_2['plan'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5zDUf8312g9"
      },
      "source": [
        "Here’s the code that was generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL6FnjLp12g9",
        "outputId": "4a6b6e2f-5ded-4fec-95b5-caa04168fa3f"
      },
      "outputs": [],
      "source": [
        "print(output_2['code'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9aBkGl312g-"
      },
      "source": [
        "Check the **score** assigned to the code and the corresponding **rationale**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zPf8OUd12g-",
        "outputId": "f25dcddc-fd27-4652-893e-d4a95672483f"
      },
      "outputs": [],
      "source": [
        "agent_result_2 = output_2['evaluation']\n",
        "agent_result_2 = extract_json(agent_result_2)\n",
        "print(f\"The score of the code is {agent_result_2['score']}\")\n",
        "print(\"\\n\")\n",
        "print(f\"The rationale for the score provided is: {agent_result_2['rationale']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfxkhTMDqXgk"
      },
      "source": [
        "Let's execute the code provided by the Agent and check if we get the desired results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMwBlQq9K7fV",
        "outputId": "44cf2148-092d-4c5a-e660-2d4e471b7a62"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('used_car_dataset.csv')\n",
        "\n",
        "# Part 1: Data Filtering\n",
        "clean_title_df = df[df['title_status'] == 'clean']\n",
        "\n",
        "# Part 2: Grouping Data\n",
        "grouped_df = clean_title_df.groupby('region')\n",
        "\n",
        "# Part 3: Calculating Average Prices\n",
        "average_prices = grouped_df['price'].mean().reset_index()\n",
        "\n",
        "# Part 4: Sorting Results\n",
        "sorted_average_prices = average_prices.sort_values(by='price', ascending=False)\n",
        "\n",
        "# Part 5: Selecting Top Regions\n",
        "top_regions = sorted_average_prices.head(10)\n",
        "\n",
        "# Part 6: Final Output Formatting\n",
        "print(top_regions.rename(columns={'price': 'Average Listing Price'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFnzsRUfqYyi"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "The agentic system explicitly decomposed the problem into logical steps, validated schema alignment, and generated correct code, ensuring consistency and clarity throughout the workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zIQ8FPuyg65"
      },
      "source": [
        "### Rigour 3: Using only vehicles with model year 2017, compute for each state:\n",
        "  - the average price of automatic-transmission vehicles\n",
        "  - the overall average price of all vehicles in that state\n",
        "  \n",
        "  Calculate the percentage deviation of automatic vehicles from the state average and return the single state with the maximum absolute deviation, along with the deviation value.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBC9_m-XVmrx"
      },
      "outputs": [],
      "source": [
        "output_3 = app.invoke({\n",
        "    \"question\": question_3,\n",
        "    \"csv_schema\": df_schema,\n",
        "    \"csv_paths\": data_path,\n",
        "    \"replan_count\" : 0,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEe08kwm22Uy"
      },
      "source": [
        "First, let’s review the plan generated by the agent to build the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX1bv4Gd22Uz",
        "outputId": "ea5505af-44db-40a5-997f-69cd01b4e997"
      },
      "outputs": [],
      "source": [
        "print(output_3['plan'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZwU-XlR22Uz"
      },
      "source": [
        "Here’s the code that was generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjMSsLcl22Uz",
        "outputId": "9b5dcb13-e233-4d99-9916-f1043956bba1"
      },
      "outputs": [],
      "source": [
        "print(output_3['code'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHdpLUBc22Uz"
      },
      "source": [
        "Check the **score** assigned to the code and the corresponding **rationale**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwkhjHAA22Uz",
        "outputId": "ca19ee40-eb62-4419-cd31-32a3c132a281"
      },
      "outputs": [],
      "source": [
        "agent_result_3 = output_3['evaluation']\n",
        "agent_result_3 = extract_json(agent_result_3)\n",
        "print(f\"The score of the code is {agent_result_3['score']}\")\n",
        "print(\"\\n\")\n",
        "print(f\"The rationale for the score provided is: {agent_result_3['rationale']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezyd1qhBq9Mi"
      },
      "source": [
        "Let's execute the code provided by the Agent and check if we get the desired results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9iULh7h3FNq",
        "outputId": "adc25038-ff24-47cf-e733-a4b68bc2dfed"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the CSV Data\n",
        "df = pd.read_csv('used_car_dataset.csv')\n",
        "\n",
        "# Step 2: Filter for Model Year 2017\n",
        "df_2017 = df[df['year'] == 2017]\n",
        "\n",
        "# Step 3: Filter for Automatic Transmission Vehicles\n",
        "df_automatic = df_2017[df_2017['transmission'] == 'automatic']\n",
        "\n",
        "# Step 4: Group by State\n",
        "grouped_automatic = df_automatic.groupby('state')['price'].mean().reset_index()\n",
        "grouped_automatic.columns = ['state', 'avg_price_automatic']\n",
        "\n",
        "grouped_all = df_2017.groupby('state')['price'].mean().reset_index()\n",
        "grouped_all.columns = ['state', 'avg_price_all']\n",
        "\n",
        "# Step 5: Merge Average Price DataFrames\n",
        "merged = pd.merge(grouped_automatic, grouped_all, on='state')\n",
        "\n",
        "# Step 6: Calculate Percentage Deviation\n",
        "merged['percentage_deviation'] = ((merged['avg_price_automatic'] - merged['avg_price_all']) / merged['avg_price_all']) * 100\n",
        "\n",
        "# Step 7: Calculate Absolute Deviation\n",
        "merged['absolute_deviation'] = merged['percentage_deviation'].abs()\n",
        "\n",
        "# Step 8: Find Maximum Absolute Deviation\n",
        "max_deviation_row = merged.loc[merged['absolute_deviation'].idxmax()]\n",
        "\n",
        "# Step 9: Prepare Final Output\n",
        "result = (max_deviation_row['state'], max_deviation_row['absolute_deviation'])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zzTKYWsrYP0"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "For the complex multi-step query, the agents planned all required calculations upfront, validated logical completeness, and generated fully correct code, showing strong reliability for high-rigor analytical tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYH4kl14JpWn"
      },
      "source": [
        "### Observations: Agentic AI Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVQdIRtCRbnY",
        "outputId": "dee6c822-5162-4eec-8a0b-948e0a49ba77"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame to store the scores\n",
        "scores_data = {\n",
        "    'Rigour Level': ['Rigour Level 1', 'Rigour Level 2', 'Rigour Level 3'],\n",
        "    'LLM Score': [llm_result_1['score'], llm_result_2['score'], llm_result_3['score']],\n",
        "    'Agentic Score': [agent_result_1['score'], agent_result_2['score'], agent_result_3['score']]\n",
        "}\n",
        "\n",
        "scores_df = pd.DataFrame(scores_data)\n",
        "scores_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boAgb7Q3JTyk"
      },
      "source": [
        "* Even though the outputs are the same, the **agentic workflow introduces explicit planning**, making the reasoning process transparent and auditable.\n",
        "\n",
        "* This design makes it **easy to extend at an organizational scale**, natively supporting additions like code execution, result inspection, observation collection, and iterative correction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TviMWoB-LENP"
      },
      "source": [
        "## **Conclusions and Business Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf-ZrPFtK_9q"
      },
      "source": [
        "\n",
        "* **Adopt agentic analytics for dependable decision-making:** Implement a node-based analytical workflow to ensure structured reasoning, validation, and execution instead of relying on single-prompt LLM outputs.\n",
        "\n",
        "* **Improve analysis consistency across teams:** Standardized planning and validation steps reduce dependency on individual analyst expertise and ensure uniform analytical quality.\n",
        "\n",
        "* **Reduce analytical errors and rework:** Built-in validation and replanning mechanisms prevent incorrect assumptions, schema misuse, and incomplete logic from reaching execution.\n",
        "\n",
        "* **Accelerate time-to-insight:** Automated reasoning and code generation significantly reduce turnaround time for exploratory and ad-hoc business questions.\n",
        "\n",
        "* **Increase stakeholder trust in AI outputs:** Explicit reasoning steps and approval checks make AI-generated analysis more transparent and auditable.\n",
        "\n",
        "* **Enable scalable analytics support:** The framework allows organizations to support a higher volume of analytical queries without proportionally increasing analyst headcount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulTEcujCLKFB"
      },
      "source": [
        "<font size=5> Future Enhancements</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Hj8aZwLMvM"
      },
      "source": [
        "\n",
        "* **Human-in-the-loop plan approval:** Introduce optional human validation checkpoints before execution for high-impact or business-critical analyses.\n",
        "\n",
        "* **Natural language insight generation:** Extend outputs beyond code to include business-focused summaries, key findings, and actionable recommendations.\n",
        "\n",
        "* **End-to-end orchestration and logging:** Add centralized logging, versioning, and traceability to support enterprise-grade governance and compliance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SYqV0q15pX1"
      },
      "source": [
        "## **Data Analysis Code Generation: Agentic AI Implementation with Tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjE8feAnK7_c"
      },
      "source": [
        "### Agent State"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT6dSv_ZKZdJ"
      },
      "source": [
        "We extend the `AgentState` to explicitly store both analytical interpretation and execution results produced during the agent workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mJ45a8PHKHf"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    csv_schema: str\n",
        "    csv_paths: dict\n",
        "    plan: str\n",
        "    user_approved: bool\n",
        "    code: str\n",
        "    reasoning: str\n",
        "    replan_count: int\n",
        "    evaluation: str\n",
        "    analysis: str\n",
        "    code_output: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bEDGlZeLAaO"
      },
      "source": [
        "### Analysis Generation Node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTf8ZZ46K6VQ"
      },
      "source": [
        "This step executes the generated code, captures the raw execution output, and uses it to generate a grounded final answer to the user’s question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCqglUe3594r"
      },
      "outputs": [],
      "source": [
        "# PythonREPL provides a controlled environment to execute Python code dynamically,\n",
        "# allowing the agent to safely run generated code during the workflow\n",
        "python_repl = PythonREPL()\n",
        "\n",
        "def analysis_generation(state: AgentState):\n",
        "  code_output = python_repl.run(state[\"code\"])\n",
        "\n",
        "  state['code_output'] = code_output\n",
        "\n",
        "  analysis_prompt=f\"\"\"\n",
        "\n",
        "  ###ROLE\n",
        "  You are a data reasoning and explanation agent responsible for answering a user’s question by interpreting executed code and its output using the provided dataframe schema.\n",
        "\n",
        "  ###INPUT\n",
        "\n",
        "  User Question:\n",
        "  {state['question']}\n",
        "  DataFrame Schema (single source of truth):\n",
        "  {state['csv_schema']}\n",
        "  Executed Code:\n",
        "  {state['code']}\n",
        "  Execution Output:\n",
        "  {code_output}\n",
        "\n",
        "  ###OBJECTIVE\n",
        "\n",
        "  Provide a clear, accurate, and detailed answer to the user question by grounding your explanation strictly in the dataframe schema, executed code, and execution output.\n",
        "\n",
        "  ###INSTRUCTIONS\n",
        "\n",
        "  - Understand the user question and analytical intent.\n",
        "  - Interpret what the code does and what the output shows.\n",
        "  - Explain the output based on the question.\n",
        "  - Do not assume missing data or add external information.\n",
        "  - Clearly state if the output is insufficient to answer the question.\n",
        "\n",
        "  ###OUTPUT\n",
        "\n",
        "  Return only the final answer to the user question, written in clear, complete sentences.\n",
        "  Do not include explanations of code, processing steps, or analysis workflow.\n",
        "  \"\"\"\n",
        "\n",
        "  analysis_output = llm.invoke(analysis_prompt).content.strip()\n",
        "\n",
        "  state['analysis'] = analysis_output\n",
        "\n",
        "  return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN4LdtE9LdNN"
      },
      "source": [
        "### Define Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeGx09YiLQQ0"
      },
      "source": [
        "We add the `analysis_generation` node to produce the final grounded answer and redefine the workflow to include this step after code evaluation.\n",
        "The updated graph ensures the pipeline ends with a validated execution output and a user-ready analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsCqb7BW591H"
      },
      "outputs": [],
      "source": [
        "graph = StateGraph(AgentState)\n",
        "\n",
        "# Add all nodes\n",
        "graph.add_node(\"planner\", planner)\n",
        "graph.add_node(\"plan_validator\", plan_validator)\n",
        "graph.add_node(\"replanner\", replanner)\n",
        "graph.add_node(\"plan_to_code\", plan_to_code)\n",
        "graph.add_node(\"code_evaluator\", code_evaluator)\n",
        "graph.add_node(\"analysis_generation\", analysis_generation)\n",
        "\n",
        "# Set entry point\n",
        "graph.set_entry_point(\"planner\")\n",
        "\n",
        "# Conditional routing after planner\n",
        "graph.add_edge(\"planner\", \"plan_validator\")\n",
        "\n",
        "# Conditional routing after plan_validator\n",
        "graph.add_conditional_edges(\n",
        "    \"plan_validator\",\n",
        "    lambda state: \"approved\" if state[\"user_approved\"] else \"replan\",\n",
        "    {\n",
        "        \"approved\": \"plan_to_code\",\n",
        "        \"replan\": \"replanner\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# After replanner, re-validate the new plan\n",
        "graph.add_edge(\"replanner\", \"plan_validator\")\n",
        "\n",
        "# Pass the generated code to be evaluated and scored for quality and correctness\n",
        "graph.add_edge(\"plan_to_code\", \"code_evaluator\")\n",
        "\n",
        "# After scoring, forward the evaluated code to be executed and converted into insights\n",
        "graph.add_edge(\"code_evaluator\", \"analysis_generation\")\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGw8cA_xLmSn"
      },
      "source": [
        "## **Test Cases: Agentic AI Implementation with Tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si_MlJZmLyKh"
      },
      "source": [
        "With the updated workflow, we now obtain the final analysis for each question, generated directly from the executed code output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxkLjJL0Fzxr"
      },
      "source": [
        "### Rigour 1: How does the average listing price vary across different manufacturers for vehicles manufactured after 2015?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOpr_zBHFzxr"
      },
      "outputs": [],
      "source": [
        "output_1 = app.invoke({\n",
        "    \"question\": question_1,\n",
        "    \"csv_schema\": df_schema,\n",
        "    \"csv_paths\": data_path,\n",
        "     \"replan_count\" : 0,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjvfAmVtF9zv",
        "outputId": "c2a3def4-9207-4023-b8f7-9a37a2e66fe3"
      },
      "outputs": [],
      "source": [
        "print(output_1['analysis'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxtAw8FHL9nL"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "The answer is clear, data-grounded, and directly addresses the question with accurate comparisons and a concise summary of key insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfi0JWF5Fzxs"
      },
      "source": [
        "### Rigour 2: Which regions have the highest average listing prices for vehicles with a clean title status?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEiz58a9Fzxt"
      },
      "outputs": [],
      "source": [
        "output_2 = app.invoke({\n",
        "    \"question\": question_2,\n",
        "    \"csv_schema\": df_schema,\n",
        "    \"csv_paths\": data_path,\n",
        "    \"replan_count\" : 0,\n",
        "\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu4_RUIUGA6H",
        "outputId": "f3004ff7-7a64-4ea1-b07d-81947a236d60"
      },
      "outputs": [],
      "source": [
        "print(output_2['analysis'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNGbo7VrMNi-"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "The answer is well-structured, clearly ranked, and directly supported by the reported averages, making it easy to identify regions with the highest clean-title vehicle prices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bok3kFZVFzxu"
      },
      "source": [
        "### Rigour 3: Using only vehicles with model year 2017, compute for each state:\n",
        "  - the average price of automatic-transmission vehicles\n",
        "  - the overall average price of all vehicles in that state\n",
        "  \n",
        "  Calculate the percentage deviation of automatic vehicles from the state average and return the single state with the maximum absolute deviation, along with the deviation value.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95lLQ13zGFga"
      },
      "outputs": [],
      "source": [
        "output_3 = app.invoke({\n",
        "    \"question\": question_3,\n",
        "    \"csv_schema\": df_schema,\n",
        "    \"csv_paths\": data_path,\n",
        "    \"replan_count\" : 0,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNptkSBPEb7S",
        "outputId": "a26fe289-b05c-4e41-ca37-ebbbb7a498ba"
      },
      "outputs": [],
      "source": [
        "print(output_3['analysis'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKc0c05vMSvI"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "The answer is concise, directly addresses the question, and clearly reports both the identified state and the computed deviation value without unnecessary detail.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUmWdQYw5MVt"
      },
      "source": [
        "<font size=5>**Note**: In the next week, we'll explore different kinds of tools that AI Agents can utilize to execute certain actions, enabling them to perform more complex tasks in a streamlined manner.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgQX4D2t5Fjv"
      },
      "source": [
        "<font size=6 color='blue'>Power Ahead!</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3CNz35ia6Bz3",
        "by9EvAnkSpZf",
        "lnwETBOE6Bz5",
        "Uq1lhM4WFTS2",
        "0G2bFup5u6vE",
        "krZUb4K8u8LF",
        "ENVRr6dJyrnQ",
        "NOAtqOw7GNwq",
        "QehZ2YZsH3sv",
        "cun6YyRX3Ic5",
        "HpFhnRwg6Epu",
        "y7kgFaW1FV-P",
        "x6vYb5xJFaEW",
        "2kizft_mFd6g",
        "jEwN_q1neeaq",
        "LRUJfuGrFw-Z",
        "JYH4kl14JpWn",
        "TviMWoB-LENP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
